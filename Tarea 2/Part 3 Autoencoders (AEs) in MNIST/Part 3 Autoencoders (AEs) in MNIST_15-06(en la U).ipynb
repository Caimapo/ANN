{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los datos desde keras, se normalizan las imagenes de modo que los pixeles queden descritos en el range [0,1], luegos se transforman en vectores unidimensionales y se parten en conjuntos disjuntos de entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 24s 2us/step\n",
      "datos de entrenamiento : (50000, 28, 28)\n",
      "datos de validacion : (10000, 28, 28)\n",
      "datos de pruebas : (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255. #and x_test\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "#Se extraen datos para hacer el set de validación\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size=(1/6))\n",
    "\n",
    "print('datos de entrenamiento : '+str(x_train.shape))\n",
    "print('datos de validacion : '+str(x_val.shape))\n",
    "print('datos de pruebas : '+str(x_test.shape))\n",
    "\n",
    "#como vector\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_val = np_utils.to_categorical(y_val, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se pasa a usar los datos para una red de autoencoder feed forward, en donde las capas de este son densas. Para esto se re estructuraran los datos de entradas en forma de vector, es decir la matriz de 28 × 28 pasa a ser un vector de 784 componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "X_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "X_val = x_val.reshape((len(x_val), np.prod(x_val.shape[1:])))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección a) Se entrena un AE (1 hddn layer) para generar una representación de MNIST de d' = 2 ,8 ,32 ,64 . Se elije la función de perdida categorical_crossentropy   y el criterio del entrenamiento sigmoid . Determine el porcentaje de compresión obtneido y el error de reconstrucción en cada caso. Se mojra el resulta si elegiumos un funcióin de activación reulo para el encoder o decoder.\n",
    "\n",
    "** revisar si binary-crossentroy o categorical (condiderando que trabaja con múltiples clases)\n",
    "\n",
    "** Falta % de compresión, a que se refiere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_a(c1,c2,name):\n",
    "    ## Completar para mostrar distintos hist para graficar la perdida. \n",
    "    ## Mostrar la matriz de confusión para cada configuración\n",
    "    ## sacar conclusiones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  import sys\n",
      "/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n",
      "/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 591.2862 - val_loss: 578.5116\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 570.1969 - val_loss: 568.4323\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 559.3611 - val_loss: 556.8852\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 553.6512 - val_loss: 556.7777\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 11s 225us/step - loss: 552.0692 - val_loss: 553.4816\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 550.3885 - val_loss: 551.3231\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 548.4762 - val_loss: 549.2239\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 547.5388 - val_loss: 547.9952\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 546.1104 - val_loss: 547.3593\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 544.7292 - val_loss: 545.2160\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 543.6927 - val_loss: 544.7438\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 543.3006 - val_loss: 545.6080\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 11s 212us/step - loss: 543.0709 - val_loss: 545.4541\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 542.8785 - val_loss: 543.5822\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 541.9012 - val_loss: 542.9329\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 541.1847 - val_loss: 541.6437\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 539.9880 - val_loss: 540.8694\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 539.6005 - val_loss: 540.9584\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 539.0785 - val_loss: 539.6608\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 538.5163 - val_loss: 540.3076\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 537.7616 - val_loss: 539.0916\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 537.5059 - val_loss: 539.3063\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 11s 226us/step - loss: 537.3786 - val_loss: 538.8348\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 11s 227us/step - loss: 537.3088 - val_loss: 538.6172\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 537.2670 - val_loss: 538.3843\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 537.1698 - val_loss: 538.1017\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 12s 239us/step - loss: 537.0189 - val_loss: 538.0939\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 11s 229us/step - loss: 536.4261 - val_loss: 537.1988\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 535.9248 - val_loss: 537.9045\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 535.5471 - val_loss: 536.8056\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 535.3556 - val_loss: 536.4156\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 534.8619 - val_loss: 535.9578\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 534.6667 - val_loss: 536.0680\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 534.5625 - val_loss: 536.0576\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 534.4737 - val_loss: 536.2336\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 534.4023 - val_loss: 535.4101\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 534.3507 - val_loss: 535.4142\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 534.2786 - val_loss: 535.4635\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 534.1655 - val_loss: 536.0264\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 11s 224us/step - loss: 533.7523 - val_loss: 534.9346\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 12s 231us/step - loss: 533.5569 - val_loss: 534.9664\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 533.4378 - val_loss: 534.7652\n",
      "Epoch 43/50\n",
      "22464/50000 [============>.................] - ETA: 6s - loss: 534.3549"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-01b6f8c4df51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic_autoencoder_768x32.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#save other stuffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/redes-neuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(32, activation='sigmoid')(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "encoder = Model(input=input_img, output=encoded)\n",
    "encoded_input = Input(shape=(32,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "##\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='categorical_crossentropy')\n",
    "hist = autoencoder.fit(X_train,X_train,epochs=50,batch_size=32,validation_data=(X_val,X_val))\n",
    "autoencoder.save('basic_autoencoder_768x32_sigmoid.h5')\n",
    "#save other stuffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(32, activation='relu')(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "encoder = Model(input=input_img, output=encoded)\n",
    "encoded_input = Input(shape=(32,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "##\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='categorical_crossentropy')\n",
    "hist = autoencoder.fit(X_train,X_train,epochs=50,batch_size=32,validation_data=(X_val,X_val))\n",
    "autoencoder.save('basic_autoencoder_768x32_eRelu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(32, activation='sigmoid')(input_img)\n",
    "decoded = Dense(784, activation='relu')(encoded)\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "encoder = Model(input=input_img, output=encoded)\n",
    "encoded_input = Input(shape=(32,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "##\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='categorical_crossentropy')\n",
    "hist = autoencoder.fit(X_train,X_train,epochs=50,batch_size=32,validation_data=(X_val,X_val))\n",
    "autoencoder.save('basic_autoencoder_768x32_sRelu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(32, activation='relu')(input_img)\n",
    "decoded = Dense(784, activation='relu')(encoded)\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "encoder = Model(input=input_img, output=encoded)\n",
    "encoded_input = Input(shape=(32,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "##\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='categorical_crossentropy')\n",
    "hist = autoencoder.fit(X_train,X_train,epochs=50,batch_size=32,validation_data=(X_val,X_val))\n",
    "autoencoder.save('basic_autoencoder_768x32_Relu.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección b) Se comparan visialmente la reconstrucción que logra realizar el autoencoder. Determinar si la percepción visual se corresponde con el error de reconstrucción que se observa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## verificar que encoder y decoder sean los mejores del punto anterior.\n",
    "## mostra graficamente para las cuatro configuraciones del punto anterior\n",
    "## seria interesante ver la imagen pequeña\n",
    "\n",
    "from keras.models import load_model\n",
    "autoencoder = load_model('basic_autoencoder_768x32.h5')\n",
    "#load other stuff ...\n",
    "encoded_test = encoder.predict(X_test)\n",
    "decoded_test = decoder.predict(encoded_test)\n",
    "import matplotlib\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_test[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección c) Se verifica la calidad de la representación obtenida, para ello se construye un clasificador kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy PCA 0.90\n",
      "Classification Accuracy 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pca = PCA(n_components=10)\n",
    "#PCA\n",
    "pca.fit(X_train)\n",
    "pca_train = pca.transform(X_train)\n",
    "pca_test = pca.transform(X_test)\n",
    "#AUTOENCODER\n",
    "encoded_train = encoder.predict(X_train)\n",
    "encoded_test = encoder.predict(X_test)\n",
    "#CLASIFICATION\n",
    "clf = KNeighborsClassifier(10)\n",
    "clf.fit(pca_train, Y_train)\n",
    "print ('Classification Accuracy PCA %.2f' % clf.score(pca_test,Y_test))\n",
    "clf = KNeighborsClassifier(10)\n",
    "clf.fit(encoded_train, Y_train)\n",
    "print ('Classification Accuracy %.2f' % clf.score(encoded_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falta colocar la conclusión de que AE es peor de KNN (sería interesante poder justificar porque?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección d) Deep encoding, se experimenta el uso de más de dos capas ocultas para demostrar que mejora la compresión obtenida por PCA, utilizando el mismo número de dimensiones. Para ello se prueba con $d \\in [2,4,8,16]$ y distintas profundidades $L \\in [2,3,4]$ y se compara el desempeño tanto en el error de reconstrucción como en clasificación via kNN de cada representación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 2 #try other and do a nice plot\n",
    "input_img = Input(shape=(784,))\n",
    "encoded1 = Dense(1000, activation='relu')(input_img)\n",
    "encoded2 = Dense(500, activation='relu')(encoded1)\n",
    "encoded3 = Dense(250, activation='relu')(encoded2)\n",
    "encoded4 = Dense(target_dim, activation='relu')(encoded3)\n",
    "decoded4 = Dense(250, activation='relu')(encoded4)\n",
    "decoded3 = Dense(500, activation='relu')(encoded3)\n",
    "decoded2 = Dense(1000, activation='relu')(decoded3)\n",
    "decoded1 = Dense(784, activation='sigmoid')(decoded2)\n",
    "autoencoder = Model(input=input_img, output=decoded1)\n",
    "encoder = Model(input=input_img, output=encoded3)\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,epochs=40,batch_size=32,validation_data=(x_val,x_val))\n",
    "autoencoder.save('my_autoencoder_768x1000x500x250x2.h5')\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pca = PCA(n_components=target_dim)\n",
    "pca.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver que configuración usar, con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección e) Se elije una de las representaciones anteriores y se visualizan usando la herramienta TSNE disponible en la librería sklearn. Para proceder con una comparación cualitativa de los resultados obtenidos mediante PCA y AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplot=5000 #warning: mind your memory!\n",
    "encoded_train = encoder.predict(x_train[:nplot])\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "encoded_train = model.fit_transform(encoded_train)\n",
    "plt.figure(figsize=(10, 10))\n",
    "colors={0:'b',1:'g',2:'r',3:'c',4:'m',5:'y',6:'k',7:'orange',8:'darkgreen',9:'maroon'}\n",
    "markers={0:'o',1:'+',2: 'v',3:'<',4:'>',5:'^',6:'s',7:'p',8:'*',9:'x'}\n",
    "for idx in xrange(0,nplot):\n",
    "    label = y_train[idx]\n",
    "    line = plt.plot(encoded_train[idx][0], encoded_train[idx][1],\n",
    "        color=colors[label], marker=markers[label], markersize=6)\n",
    "pca_train = pca.transform(x_train)\n",
    "encoded_train = pca_train[:nplot]\n",
    "... #plot PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección f) Por ultimo se modifica el primer autoencoder (a) para trabajar directamente sobre las  imágenes de MNIST, sin tratarlas como vectores, sino como matrices de tamaño 1 x 28 x 28. Es posible utilizando capas convolucionales para definir el Encoder y el Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = x_train.astype('float32') / 255.\n",
    "X_test_ = x_test.astype('float32') / 255.\n",
    "X_train_ = np.reshape(x_train, (len(x_train), 28, 28, 1)) #modify for th dim ordering\n",
    "X_test_ = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "input_img_ = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img_)\n",
    "x = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
